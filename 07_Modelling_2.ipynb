{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Imports](#imports)\n",
    "- [Modelling](#model)\n",
    "- [Holdout Testing](#holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports<a id=imports></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, make_scorer, roc_curve, classification_report, precision_recall_curve, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our data from our feature engineered csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'.\\data\\feateng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into our X and y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=data[[x for x in data.columns if x != 'move']]\n",
    "y=data['move']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use SMOTE to upsample our minority classes to ensure an even distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21072 21072\n"
     ]
    }
   ],
   "source": [
    "print(str(len(X_res))+' '+str(len(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our train-test-split on our X and y data, stratifying on our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_res,y_res,random_state=42,stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we proceed with our modelling in the same manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code found from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=-1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_models = {\n",
    "    'LogisticRegression' : LogisticRegression(random_state = 42),\n",
    "    'KNN': KNeighborsClassifier(), \n",
    "#     'NaiveBayes' : MultinomialNB(),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42), \n",
    "    'BaggedDecisionTree' : BaggingClassifier(random_state = 42),\n",
    "    'RandomForest' : RandomForestClassifier(random_state = 42), \n",
    "    'ExtraTrees' : ExtraTreesClassifier(random_state = 42), \n",
    "    'AdaBoost' : AdaBoostClassifier(random_state=42), \n",
    "    'GradientBoosting' : GradientBoostingClassifier(random_state = 42),\n",
    "    'SVM' : SVC(random_state=42),\n",
    "    'XGBoost' : XGBClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_model_params = {\n",
    "    'LogisticRegression' : {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.arange(.05, 1, .05) },\n",
    "    'KNN' : {\n",
    "        'n_neighbors' : np.arange(3, 22, 2) },\n",
    "    'NaiveBayes' : {\n",
    "        'alpha' : np.arange(.05, 2, .05)},\n",
    "    'DecisionTree': {\n",
    "        'max_depth' : [None, 6, 10, 14], \n",
    "        'min_samples_leaf' : [1, 2],\n",
    "        'min_samples_split': [2, 3] },\n",
    "    'BaggedDecisionTree' : {\n",
    "        'n_estimators' : [20, 60, 100] },\n",
    "    'RandomForest' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 2, 6, 10],\n",
    "        'min_samples_split' : [2, 3, 4] },\n",
    "    'ExtraTrees' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 6, 10, 14],\n",
    "        'min_samples_leaf' : [1, 2], \n",
    "        'min_samples_split' : [2, 3], },\n",
    "    'AdaBoost' : {\n",
    "        'n_estimators' : np.arange(100, 151, 25),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10) },\n",
    "    'GradientBoosting' : {\n",
    "        'n_estimators' : np.arange(5, 150, 15),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10),\n",
    "        'max_depth' : [1, 2, 3] },\n",
    "    'SVM' : {\n",
    "        'C' : np.arange(0.05, 1, .05),\n",
    "        'kernel' : ['rbf', 'linear'] },\n",
    "    'XGBoost' : {\n",
    "        'n_estimators'  : np.arange(100, 151, 25), \n",
    "        'learning_rate' : np.arange(0.1, 1, .3),\n",
    "        'max_depth' : [3],\n",
    "        'alpha' : np.arange(0, 1, .3),\n",
    "        'lambda' : np.arange(0, 1, .3),\n",
    "        'gamma' : np.arange(0, 1, .3),\n",
    "        'subsample' : [.5],\n",
    "        'n_jobs' : [4],}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for KNN.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for DecisionTree.\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BaggedDecisionTree.\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    6.9s remaining:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   12.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   17.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ExtraTrees.\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoost.\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   57.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoosting.\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 19.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for SVM.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBoost.\n",
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed: 56.9min finished\n"
     ]
    }
   ],
   "source": [
    "search = EstimatorSelectionHelper(classifier_models, classifier_model_params)\n",
    "search.fit(X_train, y_train, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Holdout Testing<a id=holdout></a>\n",
    "We create the table of our GridSearch scores and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "KNN\n",
      "DecisionTree\n",
      "BaggedDecisionTree\n",
      "RandomForest\n",
      "ExtraTrees\n",
      "AdaBoost\n",
      "GradientBoosting\n",
      "SVM\n",
      "XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score1=search.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the pivot table which gives us the best scores and the parameters for each of our estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTrees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996963</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997153</td>\n",
       "      <td>0.996014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996393</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>0.995191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.996203</td>\n",
       "      <td>0.995001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.993356</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggedDecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990319</td>\n",
       "      <td>0.988547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.981587</td>\n",
       "      <td>0.979879</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.977031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.906036</td>\n",
       "      <td>0.900658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.897874</td>\n",
       "      <td>0.892875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823083</td>\n",
       "      <td>0.808719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.799734</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044687</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       C  alpha  gamma kernel  lambda  learning_rate  \\\n",
       "estimator                                                              \n",
       "ExtraTrees           NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "XGBoost              NaN    0.9    0.9    NaN     0.9            0.7   \n",
       "GradientBoosting     NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "KNN                  NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "RandomForest         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "BaggedDecisionTree   NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "DecisionTree         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "LogisticRegression  0.95    NaN    NaN    NaN     NaN            NaN   \n",
       "SVM                 0.95    NaN    NaN    rbf     NaN            NaN   \n",
       "AdaBoost             NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "\n",
       "                    max_depth  max_score  mean_score  min_samples_leaf  \\\n",
       "estimator                                                                \n",
       "ExtraTrees               14.0   0.998102    0.997216               2.0   \n",
       "XGBoost                   3.0   0.997153    0.996014               NaN   \n",
       "GradientBoosting          3.0   0.996393    0.995634               NaN   \n",
       "KNN                       NaN   0.995634    0.995191               NaN   \n",
       "RandomForest             10.0   0.996203    0.995001               NaN   \n",
       "BaggedDecisionTree        NaN   0.990319    0.988547               NaN   \n",
       "DecisionTree             14.0   0.981587    0.979879               2.0   \n",
       "LogisticRegression        NaN   0.906036    0.900658               NaN   \n",
       "SVM                       NaN   0.897874    0.892875               NaN   \n",
       "AdaBoost                  NaN   0.823083    0.808719               NaN   \n",
       "\n",
       "                    min_samples_split  min_score  n_estimators  n_jobs  \\\n",
       "estimator                                                                \n",
       "ExtraTrees                        3.0   0.996963         100.0     NaN   \n",
       "XGBoost                           NaN   0.994685         150.0     4.0   \n",
       "GradientBoosting                  NaN   0.994875         140.0     NaN   \n",
       "KNN                               NaN   0.994685           NaN     NaN   \n",
       "RandomForest                      4.0   0.993356         100.0     NaN   \n",
       "BaggedDecisionTree                NaN   0.985763         100.0     NaN   \n",
       "DecisionTree                      3.0   0.977031           NaN     NaN   \n",
       "LogisticRegression                NaN   0.894267           NaN     NaN   \n",
       "SVM                               NaN   0.888952           NaN     NaN   \n",
       "AdaBoost                          NaN   0.799734         150.0     NaN   \n",
       "\n",
       "                    n_neighbors penalty  std_score  subsample  \n",
       "estimator                                                      \n",
       "ExtraTrees                  NaN     NaN   0.005785        NaN  \n",
       "XGBoost                     NaN     NaN   0.001591        0.5  \n",
       "GradientBoosting            NaN     NaN   0.027797        NaN  \n",
       "KNN                        21.0     NaN   0.001886        NaN  \n",
       "RandomForest                NaN     NaN   0.008713        NaN  \n",
       "BaggedDecisionTree          NaN     NaN   0.002757        NaN  \n",
       "DecisionTree                NaN     NaN   0.017613        NaN  \n",
       "LogisticRegression          NaN      l2   0.006514        NaN  \n",
       "SVM                         NaN     NaN   0.004744        NaN  \n",
       "AdaBoost                    NaN     NaN   0.044687        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.pivot_table(score1,index='estimator',aggfunc='max').sort_values('mean_score',ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export these parameters to another external csv file for future reference if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table.to_csv(r'.\\data\\model2params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTrees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996963</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997153</td>\n",
       "      <td>0.996014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996393</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027797</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>0.995191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.996203</td>\n",
       "      <td>0.995001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.993356</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggedDecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990319</td>\n",
       "      <td>0.988547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.981587</td>\n",
       "      <td>0.979879</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.977031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.906036</td>\n",
       "      <td>0.900658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.897874</td>\n",
       "      <td>0.892875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823083</td>\n",
       "      <td>0.808719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.799734</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044687</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       C  alpha  gamma kernel  lambda  learning_rate  \\\n",
       "estimator                                                              \n",
       "ExtraTrees           NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "XGBoost              NaN    0.9    0.9    NaN     0.9            0.7   \n",
       "GradientBoosting     NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "KNN                  NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "RandomForest         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "BaggedDecisionTree   NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "DecisionTree         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "LogisticRegression  0.95    NaN    NaN    NaN     NaN            NaN   \n",
       "SVM                 0.95    NaN    NaN    rbf     NaN            NaN   \n",
       "AdaBoost             NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "\n",
       "                    max_depth  max_score  mean_score  min_samples_leaf  \\\n",
       "estimator                                                                \n",
       "ExtraTrees               14.0   0.998102    0.997216               2.0   \n",
       "XGBoost                   3.0   0.997153    0.996014               NaN   \n",
       "GradientBoosting          3.0   0.996393    0.995634               NaN   \n",
       "KNN                       NaN   0.995634    0.995191               NaN   \n",
       "RandomForest             10.0   0.996203    0.995001               NaN   \n",
       "BaggedDecisionTree        NaN   0.990319    0.988547               NaN   \n",
       "DecisionTree             14.0   0.981587    0.979879               2.0   \n",
       "LogisticRegression        NaN   0.906036    0.900658               NaN   \n",
       "SVM                       NaN   0.897874    0.892875               NaN   \n",
       "AdaBoost                  NaN   0.823083    0.808719               NaN   \n",
       "\n",
       "                    min_samples_split  min_score  n_estimators  n_jobs  \\\n",
       "estimator                                                                \n",
       "ExtraTrees                        3.0   0.996963         100.0     NaN   \n",
       "XGBoost                           NaN   0.994685         150.0     4.0   \n",
       "GradientBoosting                  NaN   0.994875         140.0     NaN   \n",
       "KNN                               NaN   0.994685           NaN     NaN   \n",
       "RandomForest                      4.0   0.993356         100.0     NaN   \n",
       "BaggedDecisionTree                NaN   0.985763         100.0     NaN   \n",
       "DecisionTree                      3.0   0.977031           NaN     NaN   \n",
       "LogisticRegression                NaN   0.894267           NaN     NaN   \n",
       "SVM                               NaN   0.888952           NaN     NaN   \n",
       "AdaBoost                          NaN   0.799734         150.0     NaN   \n",
       "\n",
       "                    n_neighbors penalty  std_score  subsample  \n",
       "estimator                                                      \n",
       "ExtraTrees                  NaN     NaN   0.005785        NaN  \n",
       "XGBoost                     NaN     NaN   0.001591        0.5  \n",
       "GradientBoosting            NaN     NaN   0.027797        NaN  \n",
       "KNN                        21.0     NaN   0.001886        NaN  \n",
       "RandomForest                NaN     NaN   0.008713        NaN  \n",
       "BaggedDecisionTree          NaN     NaN   0.002757        NaN  \n",
       "DecisionTree                NaN     NaN   0.017613        NaN  \n",
       "LogisticRegression          NaN      l2   0.006514        NaN  \n",
       "SVM                         NaN     NaN   0.004744        NaN  \n",
       "AdaBoost                    NaN     NaN   0.044687        NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.read_csv(r'.\\data\\model2params.csv',index_col='estimator')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the estimators we will use for our holdout testing based on the parameters that gave us the best accuracy stores in our GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'XGB' : XGBClassifier(reg_alpha=table.loc['XGBoost']['alpha'],reg_lambda=table.loc['XGBoost']['lambda'],random_state = 42, gamma=table.loc['XGBoost']['gamma'],learning_rate=table.loc['XGBoost']['learning_rate'],max_depth=int(table.loc['XGBoost']['max_depth']),n_estimators=int(table.loc['XGBoost']['n_estimators']),subsample=table.loc['XGBoost']['subsample']),\n",
    "    'ADA' : AdaBoostClassifier(random_state = 42,learning_rate=table.loc['AdaBoost']['learning_rate'],n_estimators=int(table.loc['AdaBoost']['n_estimators'])),\n",
    "    'GBoost' : GradientBoostingClassifier(random_state = 42,learning_rate=table.loc['GradientBoosting']['learning_rate'],max_depth=int(table.loc['GradientBoosting']['max_depth']),n_estimators=int(table.loc['GradientBoosting']['n_estimators'])),\n",
    "    'SVC' : SVC(C=table.loc['SVM']['C'],random_state = 42,kernel=table.loc['SVM']['kernel'],probability=True,gamma='auto'),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42,max_depth=int(table.loc['DecisionTree']['max_depth']),min_samples_leaf=int(table.loc['DecisionTree']['min_samples_leaf']),min_samples_split=int(table.loc['DecisionTree']['min_samples_split'])),\n",
    "    'knn' : KNeighborsClassifier(n_neighbors=int(table.loc['KNN']['n_neighbors'])),\n",
    "    'random forest' : RandomForestClassifier(random_state = 42,max_depth=int(table.loc['RandomForest']['max_depth']),min_samples_split=int(table.loc['RandomForest']['min_samples_split']),n_estimators=int(table.loc['RandomForest']['n_estimators'])),\n",
    "    'ef' : ExtraTreesClassifier(random_state = 42,max_depth=int(table.loc['ExtraTrees']['max_depth']),min_samples_leaf=int(table.loc['ExtraTrees']['min_samples_leaf']),min_samples_split=int(table.loc['ExtraTrees']['min_samples_split']),n_estimators=int(table.loc['ExtraTrees']['n_estimators'])),\n",
    "    'lr with reg' : LogisticRegression(random_state = 42,penalty=table.loc['LogisticRegression']['penalty'],C=table.loc['LogisticRegression']['C']),\n",
    "    'bagging classifier' : BaggingClassifier(random_state = 42,n_estimators=int(table.loc['BaggedDecisionTree']['n_estimators'])),\n",
    "#     'NaiveBayes' : MultinomialNB(alpha=table.loc['NaiveBayes']['alpha'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we perform our holdout testing for the previously defined estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  XGB :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       878\n",
      "           1       1.00      1.00      1.00       878\n",
      "           2       0.99      1.00      1.00       878\n",
      "           3       0.99      0.99      0.99       878\n",
      "           4       0.99      0.99      0.99       878\n",
      "           5       1.00      0.99      0.99       878\n",
      "\n",
      "    accuracy                           0.99      5268\n",
      "   macro avg       0.99      0.99      0.99      5268\n",
      "weighted avg       0.99      0.99      0.99      5268\n",
      "\n",
      "Results for  ADA :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60       878\n",
      "           1       0.94      0.97      0.95       878\n",
      "           2       0.82      0.57      0.67       878\n",
      "           3       0.74      0.24      0.37       878\n",
      "           4       0.81      0.41      0.55       878\n",
      "           5       0.35      0.97      0.52       878\n",
      "\n",
      "    accuracy                           0.61      5268\n",
      "   macro avg       0.74      0.61      0.61      5268\n",
      "weighted avg       0.74      0.61      0.61      5268\n",
      "\n",
      "Results for  GBoost :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       878\n",
      "           1       1.00      1.00      1.00       878\n",
      "           2       0.99      1.00      1.00       878\n",
      "           3       1.00      0.99      1.00       878\n",
      "           4       0.99      0.99      0.99       878\n",
      "           5       0.99      1.00      0.99       878\n",
      "\n",
      "    accuracy                           1.00      5268\n",
      "   macro avg       1.00      1.00      1.00      5268\n",
      "weighted avg       1.00      1.00      1.00      5268\n",
      "\n",
      "Results for  SVC :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70       878\n",
      "           1       0.72      1.00      0.84       878\n",
      "           2       0.77      0.87      0.81       878\n",
      "           3       0.85      0.77      0.81       878\n",
      "           4       0.98      0.76      0.86       878\n",
      "           5       0.91      0.76      0.83       878\n",
      "\n",
      "    accuracy                           0.81      5268\n",
      "   macro avg       0.82      0.81      0.81      5268\n",
      "weighted avg       0.82      0.81      0.81      5268\n",
      "\n",
      "Results for  DecisionTree :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       878\n",
      "           1       0.98      0.99      0.98       878\n",
      "           2       0.97      0.99      0.98       878\n",
      "           3       0.98      0.96      0.97       878\n",
      "           4       0.98      0.98      0.98       878\n",
      "           5       0.98      0.97      0.97       878\n",
      "\n",
      "    accuracy                           0.98      5268\n",
      "   macro avg       0.98      0.98      0.98      5268\n",
      "weighted avg       0.98      0.98      0.98      5268\n",
      "\n",
      "Results for  knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       878\n",
      "           1       0.99      1.00      0.99       878\n",
      "           2       0.98      1.00      0.99       878\n",
      "           3       0.99      0.97      0.98       878\n",
      "           4       1.00      0.98      0.99       878\n",
      "           5       0.98      0.99      0.98       878\n",
      "\n",
      "    accuracy                           0.99      5268\n",
      "   macro avg       0.99      0.99      0.99      5268\n",
      "weighted avg       0.99      0.99      0.99      5268\n",
      "\n",
      "Results for  random forest :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       878\n",
      "           1       0.99      0.99      0.99       878\n",
      "           2       0.99      1.00      0.99       878\n",
      "           3       1.00      0.99      0.99       878\n",
      "           4       1.00      0.99      0.99       878\n",
      "           5       0.99      1.00      1.00       878\n",
      "\n",
      "    accuracy                           0.99      5268\n",
      "   macro avg       0.99      0.99      0.99      5268\n",
      "weighted avg       0.99      0.99      0.99      5268\n",
      "\n",
      "Results for  ef :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       878\n",
      "           1       0.98      1.00      0.99       878\n",
      "           2       0.99      0.99      0.99       878\n",
      "           3       1.00      0.99      0.99       878\n",
      "           4       1.00      0.99      0.99       878\n",
      "           5       0.99      1.00      1.00       878\n",
      "\n",
      "    accuracy                           0.99      5268\n",
      "   macro avg       0.99      0.99      0.99      5268\n",
      "weighted avg       0.99      0.99      0.99      5268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  lr with reg :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.63      0.70       878\n",
      "           1       0.78      1.00      0.88       878\n",
      "           2       0.79      0.88      0.83       878\n",
      "           3       0.82      0.83      0.82       878\n",
      "           4       0.93      0.81      0.87       878\n",
      "           5       0.90      0.83      0.86       878\n",
      "\n",
      "    accuracy                           0.83      5268\n",
      "   macro avg       0.83      0.83      0.83      5268\n",
      "weighted avg       0.83      0.83      0.83      5268\n",
      "\n",
      "Results for  bagging classifier :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       878\n",
      "           1       0.99      0.99      0.99       878\n",
      "           2       0.99      1.00      0.99       878\n",
      "           3       0.99      0.99      0.99       878\n",
      "           4       0.99      0.99      0.99       878\n",
      "           5       0.99      0.99      0.99       878\n",
      "\n",
      "    accuracy                           0.99      5268\n",
      "   macro avg       0.99      0.99      0.99      5268\n",
      "weighted avg       0.99      0.99      0.99      5268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for e_name,estimator in estimators.items():\n",
    "    estimator.fit(X_train,y_train)\n",
    "    pred=estimator.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, estimator.predict_proba(X_test)[:,1],pos_label=1)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, estimator.predict_proba(X_test)[:,1],pos_label=1)\n",
    "    score = {'est':e_name,\n",
    "             'roc_curve':[fpr, tpr],\n",
    "             'prc':[precision,recall]}\n",
    "    scores.append(score)\n",
    "    print('Results for ',e_name,':')\n",
    "    print(classification_report(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our first-round of modelling, our models seem to perform admirably.<br/>\n",
    "XGBoost,Gradient Boosting, Random Forest, KNN, Extra Trees and Bagging Classifier models give us close to perfect accuracy and the other models also perform well with the exception of the AdaBoost model again.<br/>\n",
    "However, as our first round of modelling and testing have shown, holdout test results can be vastly different from the results of actual testing with a separate dataset.<br/><br/>\n",
    "Now that we have fitted all our best models, we shall pickle the models for testing purposes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e_name, estimator in estimators.items():\n",
    "    pickle.dump(estimator,open('./data/model2/'+str(e_name)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we test if our pickled files are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model='XGB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod=pickle.load(open(r'.//data/model2/'+model+'.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare=estimators['XGB'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "    if result[i]==compare[i]:\n",
    "        continue\n",
    "    else:\n",
    "        print('mismatch')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our pickling was successful. We shall move on to testing our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
