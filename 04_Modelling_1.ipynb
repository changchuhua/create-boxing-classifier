{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Imports](#imports)\n",
    "- [Data Processing](#process)\n",
    "- [Modelling](#model)\n",
    "- [Holdout Testing](#holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports<a id=imports></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, make_scorer, roc_curve, classification_report, precision_recall_curve, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our data that from csvs we had created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control=pd.read_csv(r'.//data/control_3.807999999999802.csv')\n",
    "guard=pd.read_csv(r'.//data/Guardclean_7.0959999999994405.csv')\n",
    "jab=pd.read_csv(r'.//data/Jabclean_1.9500000000000015.csv')\n",
    "cross=pd.read_csv(r'.//data/Crossclean_2.629999999999932.csv')\n",
    "hook=pd.read_csv(r'.//data/Hookclean_3.2439999999998643.csv')\n",
    "upcut=pd.read_csv(r'.//data/Uppercutclean_1.0720000000000007.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Processing<a id=process></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our classification labels in our target column called 'move'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control['move']=0\n",
    "guard['move']=1\n",
    "jab['move']=2\n",
    "cross['move']=3\n",
    "hook['move']=4\n",
    "upcut['move']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine all our different dataframes into one large dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>move</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>...</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   move    x0    x1  x10   x11   x12  x13   x14   x15   x16  ...  y16  y17  \\\n",
       "0     0  0.59  0.53  NaN  0.52   NaN  NaN  0.58  0.59  0.54  ...  0.3  NaN   \n",
       "1     0  0.59  0.53  NaN  0.55  0.55  NaN  0.58  0.59  0.53  ...  0.3  NaN   \n",
       "2     0  0.58  0.53  NaN  0.55  0.56  NaN  0.57  0.59  0.53  ...  0.3  NaN   \n",
       "3     0  0.58  0.52  NaN  0.54   NaN  NaN  0.57  0.58  0.53  ...  0.3  NaN   \n",
       "4     0  0.58  0.52  NaN  0.53   NaN  NaN  0.57  0.58  0.53  ...  0.3  NaN   \n",
       "\n",
       "     y2    y3    y4    y5  y6  y7    y8    y9  \n",
       "0  0.42  0.61  0.78  0.42 NaN NaN  0.75  0.96  \n",
       "1  0.43  0.61  0.75  0.42 NaN NaN  0.74  0.96  \n",
       "2  0.43  0.61  0.78  0.43 NaN NaN  0.74  0.96  \n",
       "3  0.43  0.61  0.80  0.43 NaN NaN  0.73  0.96  \n",
       "4  0.43  0.61  0.78  0.43 NaN NaN  0.74  0.96  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.concat([control,guard,jab,cross,hook,upcut],sort=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice quite a number of NaN values due to the keypoints not being captured in the videos.<br/>\n",
    "Taking this run of modelling to be the naive case, let us fill all NaN values with 0 which corresponds to the leftmost and topmost coordinates in our video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export our dataset for future use\n",
    "data.to_csv(r'./data/master.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate our dataframe into our X and y sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=data[[x for x in data.columns if x != 'move']]\n",
    "y=data['move']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use SMOTE to upsample our minority classes to ensure an even distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21288 21288\n"
     ]
    }
   ],
   "source": [
    "print(str(len(X_res))+' '+str(len(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our train-test-split on our X and y data, stratifying on our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_res,y_res,random_state=42,stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify and utilize [code found online](http://www.davidsbatista.net/blog/2018/02/23/model_optimization/) for GridSearching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code found from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=-1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the classifiers we choose to GridSearch with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_models = {\n",
    "    'LogisticRegression' : LogisticRegression(random_state = 42),\n",
    "    'KNN': KNeighborsClassifier(), \n",
    "#     'NaiveBayes' : MultinomialNB(),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42), \n",
    "    'BaggedDecisionTree' : BaggingClassifier(random_state = 42),\n",
    "    'RandomForest' : RandomForestClassifier(random_state = 42), \n",
    "    'ExtraTrees' : ExtraTreesClassifier(random_state = 42), \n",
    "    'AdaBoost' : AdaBoostClassifier(random_state=42), \n",
    "    'GradientBoosting' : GradientBoostingClassifier(random_state = 42),\n",
    "    'SVM' : SVC(random_state=42),\n",
    "    'XGBoost' : XGBClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we set the parameters for us to GridSearch through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_model_params = {\n",
    "    'LogisticRegression' : {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.arange(.05, 1, .05) },\n",
    "    'KNN' : {\n",
    "        'n_neighbors' : np.arange(3, 22, 2) },\n",
    "    'NaiveBayes' : {\n",
    "        'alpha' : np.arange(.05, 2, .05)},\n",
    "    'DecisionTree': {\n",
    "        'max_depth' : [None, 6, 10, 14], \n",
    "        'min_samples_leaf' : [1, 2],\n",
    "        'min_samples_split': [2, 3] },\n",
    "    'BaggedDecisionTree' : {\n",
    "        'n_estimators' : [20, 60, 100] },\n",
    "    'RandomForest' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 2, 6, 10],\n",
    "        'min_samples_split' : [2, 3, 4] },\n",
    "    'ExtraTrees' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 6, 10, 14],\n",
    "        'min_samples_leaf' : [1, 2], \n",
    "        'min_samples_split' : [2, 3], },\n",
    "    'AdaBoost' : {\n",
    "        'n_estimators' : np.arange(100, 151, 25),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10) },\n",
    "    'GradientBoosting' : {\n",
    "        'n_estimators' : np.arange(5, 150, 15),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10),\n",
    "        'max_depth' : [1, 2, 3] },\n",
    "    'SVM' : {\n",
    "        'C' : np.arange(0.05, 1, .05),\n",
    "        'kernel' : ['rbf', 'linear'] },\n",
    "    'XGBoost' : {\n",
    "        'n_estimators'  : np.arange(100, 151, 25), \n",
    "        'learning_rate' : np.arange(0.1, 1, .3),\n",
    "        'max_depth' : [3],\n",
    "        'alpha' : np.arange(0, 1, .3),\n",
    "        'lambda' : np.arange(0, 1, .3),\n",
    "        'gamma' : np.arange(0, 1, .3),\n",
    "        'subsample' : [.5],\n",
    "        'n_jobs' : [4],}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for KNN.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   38.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for DecisionTree.\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BaggedDecisionTree.\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    8.4s remaining:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   14.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   18.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ExtraTrees.\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   17.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoost.\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoosting.\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 26.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for SVM.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBoost.\n",
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 53.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed: 73.7min finished\n"
     ]
    }
   ],
   "source": [
    "search = EstimatorSelectionHelper(classifier_models, classifier_model_params)\n",
    "search.fit(X_train, y_train, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Holdout Testing<a id=holdout></a>\n",
    "We create a table of our GridSearch scores and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "KNN\n",
      "DecisionTree\n",
      "BaggedDecisionTree\n",
      "RandomForest\n",
      "ExtraTrees\n",
      "AdaBoost\n",
      "GradientBoosting\n",
      "SVM\n",
      "XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score1=search.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a pivot table which gives us the best scores and the parameters for each of our estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997745</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTrees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.998058</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.998497</td>\n",
       "      <td>0.997369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.996618</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994175</td>\n",
       "      <td>0.993048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.992296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggedDecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993611</td>\n",
       "      <td>0.992296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990605</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.988350</td>\n",
       "      <td>0.987974</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.987787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.958286</td>\n",
       "      <td>0.956658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956219</td>\n",
       "      <td>0.954340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.804021</td>\n",
       "      <td>0.764249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729237</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120856</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       C  alpha  gamma kernel  lambda  learning_rate  \\\n",
       "estimator                                                              \n",
       "XGBoost              NaN    0.9    0.9    NaN     0.9            0.7   \n",
       "ExtraTrees           NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "GradientBoosting     NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "RandomForest         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "KNN                  NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "BaggedDecisionTree   NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "DecisionTree         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "LogisticRegression  0.95    NaN    NaN    NaN     NaN            NaN   \n",
       "SVM                 0.95    NaN    NaN    rbf     NaN            NaN   \n",
       "AdaBoost             NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "\n",
       "                    max_depth  max_score  mean_score  min_samples_leaf  \\\n",
       "estimator                                                                \n",
       "XGBoost                   3.0   0.998873    0.998246               NaN   \n",
       "ExtraTrees               14.0   0.998873    0.998058               2.0   \n",
       "GradientBoosting          3.0   0.998685    0.997933               NaN   \n",
       "RandomForest             10.0   0.998497    0.997369               NaN   \n",
       "KNN                       NaN   0.994175    0.993048               NaN   \n",
       "BaggedDecisionTree        NaN   0.993611    0.992296               NaN   \n",
       "DecisionTree             14.0   0.988350    0.987974               2.0   \n",
       "LogisticRegression        NaN   0.958286    0.956658               NaN   \n",
       "SVM                       NaN   0.956219    0.954340               NaN   \n",
       "AdaBoost                  NaN   0.804021    0.764249               NaN   \n",
       "\n",
       "                    min_samples_split  min_score  n_estimators  n_jobs  \\\n",
       "estimator                                                                \n",
       "XGBoost                           NaN   0.997745         150.0     4.0   \n",
       "ExtraTrees                        3.0   0.997557         100.0     NaN   \n",
       "GradientBoosting                  NaN   0.997557         140.0     NaN   \n",
       "RandomForest                      4.0   0.996618         100.0     NaN   \n",
       "KNN                               NaN   0.992296           NaN     NaN   \n",
       "BaggedDecisionTree                NaN   0.990605         100.0     NaN   \n",
       "DecisionTree                      3.0   0.987787           NaN     NaN   \n",
       "LogisticRegression                NaN   0.954904           NaN     NaN   \n",
       "SVM                               NaN   0.950582           NaN     NaN   \n",
       "AdaBoost                          NaN   0.729237         150.0     NaN   \n",
       "\n",
       "                    n_neighbors penalty  std_score  subsample  \n",
       "estimator                                                      \n",
       "XGBoost                     NaN     NaN   0.000998        0.5  \n",
       "ExtraTrees                  NaN     NaN   0.006249        NaN  \n",
       "GradientBoosting            NaN     NaN   0.008718        NaN  \n",
       "RandomForest                NaN     NaN   0.007101        NaN  \n",
       "KNN                        21.0     NaN   0.002002        NaN  \n",
       "BaggedDecisionTree          NaN     NaN   0.001602        NaN  \n",
       "DecisionTree                NaN     NaN   0.003418        NaN  \n",
       "LogisticRegression          NaN      l2   0.003386        NaN  \n",
       "SVM                         NaN     NaN   0.009283        NaN  \n",
       "AdaBoost                    NaN     NaN   0.120856        NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.pivot_table(score1,index='estimator',aggfunc='max').sort_values('mean_score',ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export these parameters to an external csv file for future reference if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table.to_csv(r'.\\data\\model1params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997745</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTrees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.998873</td>\n",
       "      <td>0.998058</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.998497</td>\n",
       "      <td>0.997369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.996618</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994175</td>\n",
       "      <td>0.993048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.992296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggedDecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993611</td>\n",
       "      <td>0.992296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990605</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.988350</td>\n",
       "      <td>0.987974</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.987787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.958286</td>\n",
       "      <td>0.956658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956219</td>\n",
       "      <td>0.954340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.804021</td>\n",
       "      <td>0.764249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729237</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120856</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       C  alpha  gamma kernel  lambda  learning_rate  \\\n",
       "estimator                                                              \n",
       "XGBoost              NaN    0.9    0.9    NaN     0.9            0.7   \n",
       "ExtraTrees           NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "GradientBoosting     NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "RandomForest         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "KNN                  NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "BaggedDecisionTree   NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "DecisionTree         NaN    NaN    NaN    NaN     NaN            NaN   \n",
       "LogisticRegression  0.95    NaN    NaN    NaN     NaN            NaN   \n",
       "SVM                 0.95    NaN    NaN    rbf     NaN            NaN   \n",
       "AdaBoost             NaN    NaN    NaN    NaN     NaN            1.0   \n",
       "\n",
       "                    max_depth  max_score  mean_score  min_samples_leaf  \\\n",
       "estimator                                                                \n",
       "XGBoost                   3.0   0.998873    0.998246               NaN   \n",
       "ExtraTrees               14.0   0.998873    0.998058               2.0   \n",
       "GradientBoosting          3.0   0.998685    0.997933               NaN   \n",
       "RandomForest             10.0   0.998497    0.997369               NaN   \n",
       "KNN                       NaN   0.994175    0.993048               NaN   \n",
       "BaggedDecisionTree        NaN   0.993611    0.992296               NaN   \n",
       "DecisionTree             14.0   0.988350    0.987974               2.0   \n",
       "LogisticRegression        NaN   0.958286    0.956658               NaN   \n",
       "SVM                       NaN   0.956219    0.954340               NaN   \n",
       "AdaBoost                  NaN   0.804021    0.764249               NaN   \n",
       "\n",
       "                    min_samples_split  min_score  n_estimators  n_jobs  \\\n",
       "estimator                                                                \n",
       "XGBoost                           NaN   0.997745         150.0     4.0   \n",
       "ExtraTrees                        3.0   0.997557         100.0     NaN   \n",
       "GradientBoosting                  NaN   0.997557         140.0     NaN   \n",
       "RandomForest                      4.0   0.996618         100.0     NaN   \n",
       "KNN                               NaN   0.992296           NaN     NaN   \n",
       "BaggedDecisionTree                NaN   0.990605         100.0     NaN   \n",
       "DecisionTree                      3.0   0.987787           NaN     NaN   \n",
       "LogisticRegression                NaN   0.954904           NaN     NaN   \n",
       "SVM                               NaN   0.950582           NaN     NaN   \n",
       "AdaBoost                          NaN   0.729237         150.0     NaN   \n",
       "\n",
       "                    n_neighbors penalty  std_score  subsample  \n",
       "estimator                                                      \n",
       "XGBoost                     NaN     NaN   0.000998        0.5  \n",
       "ExtraTrees                  NaN     NaN   0.006249        NaN  \n",
       "GradientBoosting            NaN     NaN   0.008718        NaN  \n",
       "RandomForest                NaN     NaN   0.007101        NaN  \n",
       "KNN                        21.0     NaN   0.002002        NaN  \n",
       "BaggedDecisionTree          NaN     NaN   0.001602        NaN  \n",
       "DecisionTree                NaN     NaN   0.003418        NaN  \n",
       "LogisticRegression          NaN      l2   0.003386        NaN  \n",
       "SVM                         NaN     NaN   0.009283        NaN  \n",
       "AdaBoost                    NaN     NaN   0.120856        NaN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.read_csv(r'.\\data\\model1params.csv',index_col='estimator')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the estimators we will use for our holdout testing based on the parameters that gave us the best accuracy stores in our GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'XGB' : XGBClassifier(reg_alpha=table.loc['XGBoost']['alpha'],reg_lambda=table.loc['XGBoost']['lambda'],random_state = 42, gamma=table.loc['XGBoost']['gamma'],learning_rate=table.loc['XGBoost']['learning_rate'],max_depth=int(table.loc['XGBoost']['max_depth']),n_estimators=int(table.loc['XGBoost']['n_estimators']),subsample=table.loc['XGBoost']['subsample']),\n",
    "    'ADA' : AdaBoostClassifier(random_state = 42,learning_rate=table.loc['AdaBoost']['learning_rate'],n_estimators=int(table.loc['AdaBoost']['n_estimators'])),\n",
    "    'GBoost' : GradientBoostingClassifier(random_state = 42,learning_rate=table.loc['GradientBoosting']['learning_rate'],max_depth=int(table.loc['GradientBoosting']['max_depth']),n_estimators=int(table.loc['GradientBoosting']['n_estimators'])),\n",
    "    'SVC' : SVC(C=table.loc['SVM']['C'],random_state = 42,kernel=table.loc['SVM']['kernel'],probability=True,gamma='auto'),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42,max_depth=int(table.loc['DecisionTree']['max_depth']),min_samples_leaf=int(table.loc['DecisionTree']['min_samples_leaf']),min_samples_split=int(table.loc['DecisionTree']['min_samples_split'])),\n",
    "    'knn' : KNeighborsClassifier(n_neighbors=int(table.loc['KNN']['n_neighbors'])),\n",
    "    'random forest' : RandomForestClassifier(random_state = 42,max_depth=int(table.loc['RandomForest']['max_depth']),min_samples_split=int(table.loc['RandomForest']['min_samples_split']),n_estimators=int(table.loc['RandomForest']['n_estimators'])),\n",
    "    'ef' : ExtraTreesClassifier(random_state = 42,max_depth=int(table.loc['ExtraTrees']['max_depth']),min_samples_leaf=int(table.loc['ExtraTrees']['min_samples_leaf']),min_samples_split=int(table.loc['ExtraTrees']['min_samples_split']),n_estimators=int(table.loc['ExtraTrees']['n_estimators'])),\n",
    "    'lr with reg' : LogisticRegression(random_state = 42,penalty=table.loc['LogisticRegression']['penalty'],C=table.loc['LogisticRegression']['C']),\n",
    "    'bagging classifier' : BaggingClassifier(random_state = 42,n_estimators=int(table.loc['BaggedDecisionTree']['n_estimators'])),\n",
    "#     'NaiveBayes' : MultinomialNB(alpha=table.loc['NaiveBayes']['alpha'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we perform our holdout testing for the previously defined estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  XGB :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       887\n",
      "           1       1.00      1.00      1.00       887\n",
      "           2       1.00      1.00      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       1.00      0.99      1.00       887\n",
      "           5       0.99      1.00      1.00       887\n",
      "\n",
      "    accuracy                           1.00      5322\n",
      "   macro avg       1.00      1.00      1.00      5322\n",
      "weighted avg       1.00      1.00      1.00      5322\n",
      "\n",
      "Results for  ADA :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.71      0.57       887\n",
      "           1       0.47      0.98      0.64       887\n",
      "           2       0.71      0.08      0.15       887\n",
      "           3       0.57      0.97      0.71       887\n",
      "           4       0.44      0.07      0.12       887\n",
      "           5       0.78      0.34      0.47       887\n",
      "\n",
      "    accuracy                           0.53      5322\n",
      "   macro avg       0.57      0.53      0.44      5322\n",
      "weighted avg       0.57      0.53      0.44      5322\n",
      "\n",
      "Results for  GBoost :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       887\n",
      "           1       0.99      1.00      1.00       887\n",
      "           2       1.00      0.99      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       1.00      0.99      1.00       887\n",
      "           5       0.99      1.00      0.99       887\n",
      "\n",
      "    accuracy                           1.00      5322\n",
      "   macro avg       1.00      1.00      1.00      5322\n",
      "weighted avg       1.00      1.00      1.00      5322\n",
      "\n",
      "Results for  SVC :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90       887\n",
      "           1       0.80      0.96      0.87       887\n",
      "           2       0.96      0.93      0.94       887\n",
      "           3       0.96      1.00      0.98       887\n",
      "           4       0.94      0.81      0.87       887\n",
      "           5       0.87      0.91      0.89       887\n",
      "\n",
      "    accuracy                           0.91      5322\n",
      "   macro avg       0.91      0.91      0.91      5322\n",
      "weighted avg       0.91      0.91      0.91      5322\n",
      "\n",
      "Results for  DecisionTree :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       887\n",
      "           1       0.98      1.00      0.99       887\n",
      "           2       1.00      0.99      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       0.98      0.98      0.98       887\n",
      "           5       0.98      0.97      0.97       887\n",
      "\n",
      "    accuracy                           0.99      5322\n",
      "   macro avg       0.99      0.99      0.99      5322\n",
      "weighted avg       0.99      0.99      0.99      5322\n",
      "\n",
      "Results for  knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       887\n",
      "           1       0.98      0.97      0.98       887\n",
      "           2       0.99      0.99      0.99       887\n",
      "           3       0.98      0.99      0.99       887\n",
      "           4       0.98      0.96      0.97       887\n",
      "           5       0.96      0.97      0.96       887\n",
      "\n",
      "    accuracy                           0.98      5322\n",
      "   macro avg       0.98      0.98      0.98      5322\n",
      "weighted avg       0.98      0.98      0.98      5322\n",
      "\n",
      "Results for  random forest :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       887\n",
      "           1       1.00      1.00      1.00       887\n",
      "           2       1.00      1.00      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       1.00      0.99      1.00       887\n",
      "           5       0.99      1.00      1.00       887\n",
      "\n",
      "    accuracy                           1.00      5322\n",
      "   macro avg       1.00      1.00      1.00      5322\n",
      "weighted avg       1.00      1.00      1.00      5322\n",
      "\n",
      "Results for  ef :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       887\n",
      "           1       1.00      1.00      1.00       887\n",
      "           2       1.00      1.00      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       1.00      1.00      1.00       887\n",
      "           5       1.00      1.00      1.00       887\n",
      "\n",
      "    accuracy                           1.00      5322\n",
      "   macro avg       1.00      1.00      1.00      5322\n",
      "weighted avg       1.00      1.00      1.00      5322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chang\\.conda\\envs\\opencv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  lr with reg :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.91       887\n",
      "           1       0.90      0.97      0.94       887\n",
      "           2       0.95      0.95      0.95       887\n",
      "           3       0.97      0.99      0.98       887\n",
      "           4       0.94      0.89      0.91       887\n",
      "           5       0.88      0.93      0.90       887\n",
      "\n",
      "    accuracy                           0.93      5322\n",
      "   macro avg       0.93      0.93      0.93      5322\n",
      "weighted avg       0.93      0.93      0.93      5322\n",
      "\n",
      "Results for  bagging classifier :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       887\n",
      "           1       0.99      1.00      1.00       887\n",
      "           2       1.00      1.00      1.00       887\n",
      "           3       1.00      1.00      1.00       887\n",
      "           4       1.00      0.99      0.99       887\n",
      "           5       0.99      0.99      0.99       887\n",
      "\n",
      "    accuracy                           1.00      5322\n",
      "   macro avg       1.00      1.00      1.00      5322\n",
      "weighted avg       1.00      1.00      1.00      5322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for e_name,estimator in estimators.items():\n",
    "    estimator.fit(X_train,y_train)\n",
    "    pred=estimator.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, estimator.predict_proba(X_test)[:,1],pos_label=1)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, estimator.predict_proba(X_test)[:,1],pos_label=1)\n",
    "    score = {'est':e_name,\n",
    "             'roc_curve':[fpr, tpr],\n",
    "             'prc':[precision,recall]}\n",
    "    scores.append(score)\n",
    "    print('Results for ',e_name,':')\n",
    "    print(classification_report(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing how our models perform on our holdout sets, it seems that some models are performing admirably.<br/>\n",
    "XGBoost,Gradient Boosting, Random Forest, Extra Trees and Bagging Classifier models give us close to perfect accuracy, but it brings up the issue of overfitting which we will have to test later on.<br/>\n",
    "The other models also performed well with the exception of the AdaBoost model. This might possibly be a result of restricted parameters in our GridSearch.<br/><br/>\n",
    "Now that we have fitted all our best models, we shall pickle the models for testing purposes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e_name, estimator in estimators.items():\n",
    "    pickle.dump(estimator,open('./data/model1/'+str(e_name)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we test if our pickled files are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model='XGB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod=pickle.load(open(r'.//data/model1/'+model+'.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare=estimators['XGB'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "    if result[i]==compare[i]:\n",
    "        continue\n",
    "    else:\n",
    "        print('mismatch')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our pickling was successful. We shall move on to testing our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpose",
   "language": "python",
   "name": "openpose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
